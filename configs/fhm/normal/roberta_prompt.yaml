# lightning.pytorch==2.0.0
seed_everything: 1111
trainer:
  default_root_dir: lightning_logs/roberta_prompt/fhm/normal
  accelerator: gpu
  devices: 1
  max_epochs: 30
  accumulate_grad_batches: 1
  enable_checkpointing: True
  default_root_dir: lightning_logs/roberta_prompt/fhm/normal
  callbacks: 
    - class_path: lightning.pytorch.callbacks.ModelCheckpoint
      init_args:
        dirpath: checkpoints/roberta_prompt/fhm/normal
        monitor: val_auroc
        mode: max
        save_top_k: 1
        every_n_epochs: 1
        save_last: True
    - class_path: lightning.pytorch.callbacks.EarlyStopping
      init_args:
        monitor: val_auroc
        patience: 5
        mode: max
model:
  class_path: models.roberta_prompt_model.PromptModel
  init_args:
    model_class_or_path: roberta-large
    opt: 
      UNIMODAL : False
      FEAT_DIM : 2048
      ROBERTA_DIM : 512
      FC_DROPOUT : 0.4
      MID_DIM : 512
      TRANS_LAYER : 1
      NUM_HEAD : 8
      FINE_GRIND : False
      WEIGHT_DECAY : 0.01
      LR_RATE : 1e-05
      EPS : 1e-08
      FIX_LAYERS : 0

    label_list: 
      - 205
      - 1099

data:
  class_path: datamodules.datasets.prompthate_data.MultimodalDataModule
  init_args:
    model_class_or_path: roberta-large
    shuffle_train: True
    batch_size: 4
    opt: 
      DATASET : mem
      FEW_SHOT : False
      FINE_GRIND : False
      NUM_SHOTS : 16
      DATA : /mnt/sdb/aditi/hconvert/prompthate/data ## attention
      CAPTION_PATH : /mnt/sdb/aditi/hconvert/prompthate/caption ## attention
      ROBERTA_DIM : 512
      NUM_LABELS : 2
      POS_WORD : good
      NEG_WORD : bad
      DEM_SAMP : False
      SIM_RATE : 0.5
      IMG_RATE : 0.5
      TEXT_RATE : 0.5
      CLIP_CLEAN : False
      LENGTH : 64
      TOTAL_LENGTH : 256
      PRETRAIN_DATA : conceptual
      IMG_VERSION : clean
      ADD_ENT : True
      ADD_DEM : True
      DEBUG : False
      SEED : 1111
      NUM_SAMPLE : 1