{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7586a2f3",
   "metadata": {},
   "source": [
    "# Example: FLAVA training and inference on Harm-P dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "45f5ca34",
   "metadata": {},
   "source": [
    "This notebook provides a comprehensive guide on using the MATK (Multimodal AI Toolkit) library to evaluate the performance of the FLAVA model on the Harm-P dataset."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ac430990",
   "metadata": {},
   "source": [
    "## Step 1. Configuring the dataset\n",
    "\n",
    "The dataset class helps us do the following:\n",
    "* Itemize preprocessed datasets as records.\n",
    "* Aggregate primary dataset information (images, image features) with their respective records.\n",
    "* Aggregate auxilliary information (captions, web entities, explanations) with their respective records.\n",
    "\n",
    "The dataset configurations are defined in the `configs/dataset` directory. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f42d3d-aa95-4d29-983f-f06d0d2d067c",
   "metadata": {},
   "source": [
    "**Creating Harm-P datasets**\n",
    "\n",
    "Locate the **datasets** folder. You can duplicate one of the existing dataset file (e.g., fhm.py) and ensure that the following information are provided in each record:\n",
    "- `img`\n",
    "- `id`\n",
    "- labels, such as `harm_p_intensity` and `harm_p_intensity`\n",
    "- `templated_text`\n",
    "- `templated_labels` (For language outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d3ab85-0e7e-4911-be43-4fb6a4358c39",
   "metadata": {},
   "source": [
    "**Expected Output**\n",
    "```\n",
    "def __init__():\n",
    "        self.annotations = utils._load_jsonl(annotation_filepath)\n",
    "        self._preprocess_dataset()\n",
    "\n",
    "        self.auxiliary_data = self._load_auxiliary(auxiliary_dicts)\n",
    "        self._format_input_output(\n",
    "            text_template,\n",
    "            labels_template,\n",
    "            labels_mapping\n",
    "        )\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1551b8f6-dece-4e23-8320-be4508156420",
   "metadata": {},
   "source": [
    "```\n",
    "def _preprocess_dataset(self):\n",
    "        for record in tqdm.tqdm(self.annotations, desc=\"Dataset preprocessing\"):\n",
    "            record[\"img\"] = record[\"image\"]\n",
    "            record[\"id\"] = os.path.splitext(record[\"img\"])[0]\n",
    "            del record[\"image\"]\n",
    "\n",
    "            # convert label to numeric values\n",
    "            if record[\"labels\"][0] == '':\n",
    "                continue\n",
    "            \n",
    "            record[f\"{DATASET_PREFIX}_intensity\"] = INTENSITY_MAP[record[\"labels\"][0]]\n",
    "            record[f\"{DATASET_PREFIX}_target\"] = TARGET_MAP[record[\"labels\"][1]] \\\n",
    "            if len(record[\"labels\"]) > 1 else 0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5502efd-fb1a-4d99-8779-36489c70f529",
   "metadata": {},
   "source": [
    "```\n",
    "def _format_input_output(\n",
    "        self,\n",
    "        text_template: str,\n",
    "        labels_template: str,\n",
    "        labels_mapping: dict\n",
    "    ):\n",
    "        for record in tqdm.tqdm(self.annotations, desc=\"Input/Output formatting\"):\n",
    "            # format input text template\n",
    "            input_kwargs = {\"text\": record['text']}\n",
    "            for key, data in self.auxiliary_data.items():\n",
    "                input_kwargs[key] = data[record[\"id\"]]\n",
    "            text = text_template.format(**input_kwargs)\n",
    "            record[\"templated_text\"] = text\n",
    "\n",
    "            # format output text template (for text-to-text generation)\n",
    "            if labels_mapping:\n",
    "                for cls_name, label2word in labels_mapping.items():\n",
    "                    label = record[cls_name]\n",
    "                    record[f\"templated_{cls_name}\"] = labels_template.format(\n",
    "                        label=label2word[label]\n",
    "                    )\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a651928f-b5e0-4391-baf7-c599c127700c",
   "metadata": {},
   "source": [
    "**Adding Harm-P Configuration**\n",
    "\n",
    "Locate the **configs/datasets** folder. You can duplicate one of the existing YAML configuration (e.g., `fhm.yaml`) and change the filepaths for the following keys:\n",
    "- `annotation_filepaths`\n",
    "- `image_dirs`\n",
    "- `auxiliary_dicts`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96dd4eeb-25af-4fea-995c-f76f6f8e0fb9",
   "metadata": {},
   "source": [
    "**Expected Output**\n",
    "\n",
    "```\n",
    "fhm:\n",
    "  annotation_filepaths:\n",
    "    train: /mnt/data1/datasets/memes/harmp/annotations/train_v1.jsonl\n",
    "    validate: /mnt/data1/datasets/memes/harmp/annotations/val_v1.jsonl\n",
    "    test: /mnt/data1/datasets/memes/harmp/annotations/test_v1.jsonl\n",
    "    predict: /mnt/data1/datasets/memes/harmp/annotations/test_v1.jsonl\n",
    "\n",
    "  image_dirs:\n",
    "    train: /mnt/data1/datasets/memes/harmp/images/img_clean/harmeme_images_us_pol\n",
    "    validate: /mnt/data1/datasets/memes/harmp/images/img_clean/harmeme_images_us_pol\n",
    "    test: /mnt/data1/datasets/memes/harmp/images/img_clean/harmeme_images_us_pol\n",
    "    predict: /mnt/data1/datasets/memes/harmp/images/img_clean/harmeme_images_us_pol\n",
    "\n",
    "  auxiliary_dicts:\n",
    "    train: \n",
    "      caption: /mnt/data1/datasets/memes/harmp/preprocessing/blip2_captions/harmeme_images_us_pol/\n",
    "    validate:\n",
    "      caption: /mnt/data1/datasets/memes/harmp/preprocessing/blip2_captions/harmeme_images_us_pol/\n",
    "    test: \n",
    "      caption: /mnt/data1/datasets/memes/harmp/preprocessing/blip2_captions/harmeme_images_us_pol/\n",
    "    predict: \n",
    "      caption: /mnt/data1/datasets/memes/harmp/preprocessing/blip2_captions/harmeme_images_us_pol/\n",
    "\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3703ec20",
   "metadata": {},
   "source": [
    "## Step 2. Configuring FLAVA Model\n",
    "\n",
    "The model class should inherit PyTorch's `BaseLightningModule` and contain the following functionalities:\n",
    "- *training_step*\n",
    "- *validation_step*\n",
    "- *test_step*\n",
    "- *predict_step*\n",
    "\n",
    "As FLAVA is supported within the MATK package by default, we can simply reference the model class in the model configuration. A model configuration needs to have the following key:\n",
    "- `class_path`\n",
    "- `model_class_or_path`\n",
    "- `dropout`\n",
    "- `optimizers`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "372850aa-2ff4-4858-a850-6cf05f13db34",
   "metadata": {},
   "source": [
    "**Expected Output**\n",
    "```\n",
    "class_path: models.flava.FlavaClassificationModel\n",
    "model_class_or_path: facebook/flava-full\n",
    "dropout: 0.1\n",
    "optimizers: ???\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2d3b9927",
   "metadata": {},
   "source": [
    "## Step 3: Configuring Experiments\n",
    "\n",
    "Finally, we will construct a experiment configuration, which is stored inside the **configs/experiments.yaml** file. The experiment configuration is a composition of the various configuration that was constructed earlier\n",
    "\n",
    "```\n",
    "# @package _global_\n",
    "defaults:\n",
    "  - /model: flava\n",
    "  - /dataset: \n",
    "    - harm_p\n",
    "  - /datamodule: processor_datamodule\n",
    "  - /trainer: single_gpu_trainer\n",
    "  - /metric:\n",
    "    - accuracy\n",
    "    - auroc\n",
    "  - /hydra: experiment\n",
    "  - _self_\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "684ec316-0f8a-4247-b7df-1d4b116cac56",
   "metadata": {},
   "source": [
    "The experiment configuration will **override** optional/subjective parameters such as:\n",
    "```\n",
    "model:\n",
    "  optimizers: \n",
    "  - class_path: torch.optim.Adam\n",
    "    lr: 2e-5\n",
    "\n",
    "dataset:\n",
    "  harm_p:\n",
    "    dataset_class: datasets.harm_p.ImageDataset\n",
    "    text_template: \"{text}\"\n",
    "    labels:\n",
    "      harm_p_intensity: 2\n",
    "\n",
    "datamodule:\n",
    "  processor_class_or_path: facebook/flava-full\n",
    "\n",
    "monitor_metric: validate_harm_p_intensity_average\n",
    "monitor_mode: max\n",
    "save_top_ks: 1\n",
    "\n",
    "# Experiment settings\n",
    "experiment_name: baseline/harmeme/flava\n",
    "\n",
    "# Job settings\n",
    "hydra.verbose: True\n",
    "seed_everything: 1111\n",
    "overwrite: False\n",
    "action: ???\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c6e954e8",
   "metadata": {},
   "source": [
    "## Step 4: Train the Model model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ec9d8a5-191e-4a76-9d27-546f8a8c93e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] - Setting custom seed: 1111...\n",
      "Seed set to 1111\n",
      "/home/mshee/anaconda3/envs/MATK-test/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at facebook/flava-full were not used when initializing FlavaModel: ['itm_head.pooler.dense.weight', 'mim_head.transform.dense.bias', 'image_codebook.blocks.group_2.group.block_1.res_path.path.conv_2.bias', 'image_codebook.blocks.group_4.group.block_2.res_path.path.conv_2.bias', 'mmm_text_head.bias', 'image_codebook.blocks.group_4.group.block_2.res_path.path.conv_1.weight', 'mlm_head.bias', 'image_codebook.blocks.group_2.group.block_2.res_path.path.conv_1.bias', 'mmm_text_head.transform.LayerNorm.bias', 'image_codebook.blocks.group_2.group.block_1.res_path.path.conv_3.weight', 'image_codebook.blocks.group_3.group.block_1.res_path.path.conv_2.weight', 'image_codebook.blocks.group_2.group.block_2.res_path.path.conv_4.weight', 'mmm_text_head.decoder.bias', 'image_codebook.blocks.group_4.group.block_1.res_path.path.conv_2.bias', 'image_codebook.blocks.group_2.group.block_1.id_path.bias', 'image_codebook.blocks.group_1.group.block_2.res_path.path.conv_4.bias', 'image_codebook.blocks.group_4.group.block_1.res_path.path.conv_4.bias', 'image_codebook.blocks.group_3.group.block_1.res_path.path.conv_2.bias', 'image_codebook.blocks.group_4.group.block_1.id_path.weight', 'image_codebook.blocks.group_1.group.block_1.res_path.path.conv_2.weight', 'mim_head.bias', 'image_codebook.blocks.group_3.group.block_1.res_path.path.conv_1.bias', 'image_codebook.blocks.group_1.group.block_2.res_path.path.conv_1.bias', 'image_codebook.blocks.group_3.group.block_1.id_path.weight', 'mlm_head.decoder.bias', 'mlm_head.transform.dense.bias', 'image_codebook.blocks.group_3.group.block_2.res_path.path.conv_1.bias', 'image_codebook.blocks.group_2.group.block_1.res_path.path.conv_4.weight', 'image_codebook.blocks.group_3.group.block_2.res_path.path.conv_4.weight', 'image_codebook.blocks.group_4.group.block_2.res_path.path.conv_4.weight', 'image_codebook.blocks.group_4.group.block_1.res_path.path.conv_1.weight', 'mlm_head.transform.LayerNorm.weight', 'image_codebook.blocks.group_4.group.block_1.res_path.path.conv_2.weight', 'image_codebook.blocks.group_1.group.block_1.res_path.path.conv_3.bias', 'image_codebook.blocks.group_3.group.block_1.res_path.path.conv_4.weight', 'image_codebook.blocks.group_1.group.block_2.res_path.path.conv_4.weight', 'mlm_head.transform.dense.weight', 'image_codebook.blocks.group_2.group.block_2.res_path.path.conv_3.weight', 'mlm_head.transform.LayerNorm.bias', 'image_codebook.blocks.group_1.group.block_2.res_path.path.conv_3.weight', 'image_codebook.blocks.input.weight', 'image_codebook.blocks.group_4.group.block_1.res_path.path.conv_1.bias', 'image_codebook.blocks.group_2.group.block_2.res_path.path.conv_2.bias', 'image_codebook.blocks.group_4.group.block_2.res_path.path.conv_2.weight', 'image_codebook.blocks.group_2.group.block_1.res_path.path.conv_3.bias', 'image_codebook.blocks.group_1.group.block_1.res_path.path.conv_4.weight', 'mim_head.decoder.bias', 'mmm_text_head.transform.dense.bias', 'mmm_text_head.transform.LayerNorm.weight', 'mmm_image_head.decoder.weight', 'itm_head.seq_relationship.bias', 'mmm_image_head.transform.dense.weight', 'itm_head.pooler.dense.bias', 'image_codebook.blocks.group_3.group.block_2.res_path.path.conv_2.bias', 'mim_head.transform.LayerNorm.bias', 'image_codebook.blocks.group_2.group.block_1.res_path.path.conv_4.bias', 'image_codebook.blocks.group_4.group.block_1.res_path.path.conv_3.bias', 'image_codebook.blocks.group_1.group.block_2.res_path.path.conv_3.bias', 'image_codebook.blocks.group_1.group.block_2.res_path.path.conv_2.bias', 'image_codebook.blocks.group_4.group.block_1.id_path.bias', 'image_codebook.blocks.group_2.group.block_1.id_path.weight', 'image_codebook.blocks.group_2.group.block_2.res_path.path.conv_4.bias', 'image_codebook.blocks.group_2.group.block_2.res_path.path.conv_3.bias', 'itm_head.seq_relationship.weight', 'mim_head.transform.LayerNorm.weight', 'mmm_image_head.transform.LayerNorm.weight', 'image_codebook.blocks.group_1.group.block_1.res_path.path.conv_3.weight', 'image_codebook.blocks.group_2.group.block_1.res_path.path.conv_1.weight', 'image_codebook.blocks.group_4.group.block_2.res_path.path.conv_4.bias', 'image_codebook.blocks.group_2.group.block_2.res_path.path.conv_1.weight', 'mlm_head.decoder.weight', 'image_codebook.blocks.group_4.group.block_1.res_path.path.conv_4.weight', 'image_codebook.blocks.group_1.group.block_1.res_path.path.conv_2.bias', 'image_codebook.blocks.group_4.group.block_2.res_path.path.conv_3.weight', 'image_codebook.blocks.group_1.group.block_2.res_path.path.conv_1.weight', 'image_codebook.blocks.group_2.group.block_1.res_path.path.conv_2.weight', 'image_codebook.blocks.input.bias', 'image_codebook.blocks.output.conv.weight', 'image_codebook.blocks.group_3.group.block_2.res_path.path.conv_3.bias', 'image_codebook.blocks.output.conv.bias', 'mmm_image_head.transform.dense.bias', 'image_codebook.blocks.group_2.group.block_2.res_path.path.conv_2.weight', 'image_codebook.blocks.group_4.group.block_2.res_path.path.conv_1.bias', 'mmm_image_head.transform.LayerNorm.bias', 'image_codebook.blocks.group_3.group.block_1.id_path.bias', 'image_codebook.blocks.group_1.group.block_1.res_path.path.conv_4.bias', 'image_codebook.blocks.group_3.group.block_2.res_path.path.conv_3.weight', 'mim_head.decoder.weight', 'image_codebook.blocks.group_4.group.block_2.res_path.path.conv_3.bias', 'image_codebook.blocks.group_3.group.block_2.res_path.path.conv_2.weight', 'image_codebook.blocks.group_3.group.block_2.res_path.path.conv_4.bias', 'image_codebook.blocks.group_4.group.block_1.res_path.path.conv_3.weight', 'image_codebook.blocks.group_3.group.block_1.res_path.path.conv_3.bias', 'image_codebook.blocks.group_3.group.block_1.res_path.path.conv_1.weight', 'image_codebook.blocks.group_1.group.block_2.res_path.path.conv_2.weight', 'image_codebook.blocks.group_1.group.block_1.res_path.path.conv_1.weight', 'mmm_text_head.transform.dense.weight', 'image_codebook.blocks.group_3.group.block_1.res_path.path.conv_3.weight', 'image_codebook.blocks.group_2.group.block_1.res_path.path.conv_1.bias', 'image_codebook.blocks.group_1.group.block_1.res_path.path.conv_1.bias', 'mim_head.transform.dense.weight', 'image_codebook.blocks.group_3.group.block_2.res_path.path.conv_1.weight', 'mmm_image_head.bias', 'image_codebook.blocks.group_3.group.block_1.res_path.path.conv_4.bias', 'mmm_text_head.decoder.weight', 'mmm_image_head.decoder.bias']\n",
      "- This IS expected if you are initializing FlavaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing FlavaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "[INFO] - Model's Parameters: 241357827\n",
      "[INFO] - Model's Trainable Parameters: 241357827\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "[INFO] - Training model...\n",
      "You are using a CUDA device ('NVIDIA A100 80GB PCIe') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "Dataset preprocessing: 100%|████████████| 3013/3013 [00:00<00:00, 701456.37it/s]\n",
      "Loading auxiliary info: 100%|████████████| 3013/3013 [00:00<00:00, 52967.62it/s]\n",
      "Input/Output formatting: 100%|█████████| 3013/3013 [00:00<00:00, 1700180.00it/s]\n",
      "Loading images: 100%|██████████████████████| 3013/3013 [00:10<00:00, 282.30it/s]\n",
      "Dataset preprocessing: 100%|██████████████| 177/177 [00:00<00:00, 577287.56it/s]\n",
      "Loading auxiliary info: 100%|██████████████| 177/177 [00:00<00:00, 49881.87it/s]\n",
      "Input/Output formatting: 100%|███████████| 177/177 [00:00<00:00, 1543434.11it/s]\n",
      "Loading images: 100%|████████████████████████| 177/177 [00:01<00:00, 146.42it/s]\n",
      "/home/mshee/anaconda3/envs/MATK-test/lib/python3.8/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /mnt/data1/mshee/test-repository/MATK/examples/experiments/baseline/harmeme/flava exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]\n",
      "\n",
      "  | Name                                | Type               | Params\n",
      "---------------------------------------------------------------------------\n",
      "0 | model                               | FlavaModel         | 241 M \n",
      "1 | mlps                                | ModuleList         | 1.5 K \n",
      "2 | train_harmeme_intensity_accuracy    | MulticlassAccuracy | 0     \n",
      "3 | train_harmeme_intensity_auroc       | MulticlassAUROC    | 0     \n",
      "4 | validate_harmeme_intensity_accuracy | MulticlassAccuracy | 0     \n",
      "5 | validate_harmeme_intensity_auroc    | MulticlassAUROC    | 0     \n",
      "6 | test_harmeme_intensity_accuracy     | MulticlassAccuracy | 0     \n",
      "7 | test_harmeme_intensity_auroc        | MulticlassAUROC    | 0     \n",
      "---------------------------------------------------------------------------\n",
      "241 M     Trainable params\n",
      "0         Non-trainable params\n",
      "241 M     Total params\n",
      "965.431   Total estimated model params size (MB)\n",
      "Sanity Checking DataLoader 0:   0%|                       | 0/2 [00:00<?, ?it/s]/home/mshee/anaconda3/envs/MATK-test/lib/python3.8/site-packages/transformers/modeling_utils.py:810: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Sanity Checking DataLoader 0: 100%|███████████████| 2/2 [00:00<00:00,  2.36it/s][INFO] - Epoch Results:\n",
      "\tvalidate_harmeme_intensity_accuracy: 0.546875\n",
      "\tvalidate_harmeme_intensity_auroc: 0.5992063283920288\n",
      "\tvalidate_harmeme_intensity_average: 0.5730406641960144\n",
      "\n",
      "Epoch 0: 100%|████████████████████████| 95/95 [00:50<00:00,  1.90it/s, v_num=10]\n",
      "Validation: |                                             | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|                                         | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                            | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:  17%|███▎                | 1/6 [00:00<00:01,  4.92it/s]\u001b[A\n",
      "Validation DataLoader 0:  33%|██████▋             | 2/6 [00:00<00:00,  5.23it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|██████████          | 3/6 [00:00<00:00,  5.24it/s]\u001b[A\n",
      "Validation DataLoader 0:  67%|█████████████▎      | 4/6 [00:00<00:00,  4.91it/s]\u001b[A/home/mshee/anaconda3/envs/MATK-test/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: No negative samples in targets, false positive value should be meaningless. Returning zero tensor in false positive score\n",
      "  warnings.warn(*args, **kwargs)\n",
      "/home/mshee/anaconda3/envs/MATK-test/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score\n",
      "  warnings.warn(*args, **kwargs)\n",
      "\n",
      "Validation DataLoader 0:  83%|████████████████▋   | 5/6 [00:00<00:00,  5.04it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|████████████████████| 6/6 [00:01<00:00,  5.53it/s]\u001b[A[INFO] - Epoch Results:\n",
      "\tvalidate_harmeme_intensity_accuracy: 0.6723163723945618\n",
      "\tvalidate_harmeme_intensity_auroc: 0.8082249760627747\n",
      "\tvalidate_harmeme_intensity_average: 0.7402706742286682\n",
      "\n",
      "\n",
      "Epoch 1: 100%|█| 95/95 [00:50<00:00,  1.86it/s, v_num=10, validate_harmeme_inten\u001b[A\n",
      "Validation: |                                             | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|                                         | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                            | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:  17%|███▎                | 1/6 [00:00<00:01,  4.88it/s]\u001b[A\n",
      "Validation DataLoader 0:  33%|██████▋             | 2/6 [00:00<00:00,  5.19it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|██████████          | 3/6 [00:00<00:00,  5.20it/s]\u001b[A\n",
      "Validation DataLoader 0:  67%|█████████████▎      | 4/6 [00:00<00:00,  4.79it/s]\u001b[A\n",
      "Validation DataLoader 0:  83%|████████████████▋   | 5/6 [00:01<00:00,  4.95it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|████████████████████| 6/6 [00:01<00:00,  5.44it/s]\u001b[A[INFO] - Epoch Results:\n",
      "\tvalidate_harmeme_intensity_accuracy: 0.6666666865348816\n",
      "\tvalidate_harmeme_intensity_auroc: 0.8401639461517334\n",
      "\tvalidate_harmeme_intensity_average: 0.7534153461456299\n",
      "\n",
      "\n",
      "Epoch 2: 100%|█| 95/95 [00:50<00:00,  1.87it/s, v_num=10, validate_harmeme_inten\u001b[A\n",
      "Validation: |                                             | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|                                         | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                            | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:  17%|███▎                | 1/6 [00:00<00:01,  4.90it/s]\u001b[A\n",
      "Validation DataLoader 0:  33%|██████▋             | 2/6 [00:00<00:00,  5.15it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|██████████          | 3/6 [00:00<00:00,  5.18it/s]\u001b[A\n",
      "Validation DataLoader 0:  67%|█████████████▎      | 4/6 [00:00<00:00,  4.85it/s]\u001b[A\n",
      "Validation DataLoader 0:  83%|████████████████▋   | 5/6 [00:01<00:00,  4.99it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|████████████████████| 6/6 [00:01<00:00,  5.47it/s]\u001b[A[INFO] - Epoch Results:\n",
      "\tvalidate_harmeme_intensity_accuracy: 0.7175140976905823\n",
      "\tvalidate_harmeme_intensity_auroc: 0.8223572969436646\n",
      "\tvalidate_harmeme_intensity_average: 0.7699357271194458\n",
      "\n",
      "\n",
      "Epoch 3: 100%|█| 95/95 [00:51<00:00,  1.86it/s, v_num=10, validate_harmeme_inten\u001b[A\n",
      "Validation: |                                             | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|                                         | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                            | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:  17%|███▎                | 1/6 [00:00<00:01,  4.88it/s]\u001b[A\n",
      "Validation DataLoader 0:  33%|██████▋             | 2/6 [00:00<00:00,  5.18it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|██████████          | 3/6 [00:00<00:00,  5.20it/s]\u001b[A\n",
      "Validation DataLoader 0:  67%|█████████████▎      | 4/6 [00:00<00:00,  4.87it/s]\u001b[A\n",
      "Validation DataLoader 0:  83%|████████████████▋   | 5/6 [00:01<00:00,  4.99it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|████████████████████| 6/6 [00:01<00:00,  5.48it/s]\u001b[A[INFO] - Epoch Results:\n",
      "\tvalidate_harmeme_intensity_accuracy: 0.6327683329582214\n",
      "\tvalidate_harmeme_intensity_auroc: 0.732758641242981\n",
      "\tvalidate_harmeme_intensity_average: 0.6827634572982788\n",
      "\n",
      "\n",
      "Epoch 4: 100%|█| 95/95 [00:51<00:00,  1.84it/s, v_num=10, validate_harmeme_inten\u001b[A\n",
      "Validation: |                                             | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|                                         | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                            | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:  17%|███▎                | 1/6 [00:00<00:01,  4.88it/s]\u001b[A\n",
      "Validation DataLoader 0:  33%|██████▋             | 2/6 [00:00<00:00,  5.12it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|██████████          | 3/6 [00:00<00:00,  5.14it/s]\u001b[A\n",
      "Validation DataLoader 0:  67%|█████████████▎      | 4/6 [00:00<00:00,  4.82it/s]\u001b[A\n",
      "Validation DataLoader 0:  83%|████████████████▋   | 5/6 [00:01<00:00,  4.96it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|████████████████████| 6/6 [00:01<00:00,  5.44it/s]\u001b[A[INFO] - Epoch Results:\n",
      "\tvalidate_harmeme_intensity_accuracy: 0.700564980506897\n",
      "\tvalidate_harmeme_intensity_auroc: 0.7899943590164185\n",
      "\tvalidate_harmeme_intensity_average: 0.7452796697616577\n",
      "\n",
      "\n",
      "Epoch 5: 100%|█| 95/95 [00:51<00:00,  1.83it/s, v_num=10, validate_harmeme_inten\u001b[A\n",
      "Validation: |                                             | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|                                         | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                            | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:  17%|███▎                | 1/6 [00:00<00:01,  4.87it/s]\u001b[A\n",
      "Validation DataLoader 0:  33%|██████▋             | 2/6 [00:00<00:00,  5.17it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|██████████          | 3/6 [00:00<00:00,  5.17it/s]\u001b[A\n",
      "Validation DataLoader 0:  67%|█████████████▎      | 4/6 [00:00<00:00,  4.83it/s]\u001b[A\n",
      "Validation DataLoader 0:  83%|████████████████▋   | 5/6 [00:01<00:00,  4.96it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|████████████████████| 6/6 [00:01<00:00,  5.45it/s]\u001b[A[INFO] - Epoch Results:\n",
      "\tvalidate_harmeme_intensity_accuracy: 0.7231638431549072\n",
      "\tvalidate_harmeme_intensity_auroc: 0.755652904510498\n",
      "\tvalidate_harmeme_intensity_average: 0.7394083738327026\n",
      "\n",
      "\n",
      "Epoch 6: 100%|█| 95/95 [00:51<00:00,  1.85it/s, v_num=10, validate_harmeme_inten\u001b[A\n",
      "Validation: |                                             | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|                                         | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                            | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:  17%|███▎                | 1/6 [00:00<00:01,  4.86it/s]\u001b[A\n",
      "Validation DataLoader 0:  33%|██████▋             | 2/6 [00:00<00:00,  5.17it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|██████████          | 3/6 [00:00<00:00,  5.05it/s]\u001b[A\n",
      "Validation DataLoader 0:  67%|█████████████▎      | 4/6 [00:00<00:00,  4.76it/s]\u001b[A\n",
      "Validation DataLoader 0:  83%|████████████████▋   | 5/6 [00:01<00:00,  4.90it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|████████████████████| 6/6 [00:01<00:00,  5.39it/s]\u001b[A[INFO] - Epoch Results:\n",
      "\tvalidate_harmeme_intensity_accuracy: 0.700564980506897\n",
      "\tvalidate_harmeme_intensity_auroc: 0.7832108736038208\n",
      "\tvalidate_harmeme_intensity_average: 0.7418879270553589\n",
      "\n",
      "\n",
      "Epoch 7: 100%|█| 95/95 [00:51<00:00,  1.84it/s, v_num=10, validate_harmeme_inten\u001b[A\n",
      "Validation: |                                             | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|                                         | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                            | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:  17%|███▎                | 1/6 [00:00<00:01,  4.85it/s]\u001b[A\n",
      "Validation DataLoader 0:  33%|██████▋             | 2/6 [00:00<00:00,  5.15it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|██████████          | 3/6 [00:00<00:00,  5.15it/s]\u001b[A\n",
      "Validation DataLoader 0:  67%|█████████████▎      | 4/6 [00:00<00:00,  4.81it/s]\u001b[A\n",
      "Validation DataLoader 0:  83%|████████████████▋   | 5/6 [00:01<00:00,  4.95it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|████████████████████| 6/6 [00:01<00:00,  5.44it/s]\u001b[A[INFO] - Epoch Results:\n",
      "\tvalidate_harmeme_intensity_accuracy: 0.6440678238868713\n",
      "\tvalidate_harmeme_intensity_auroc: 0.6936122179031372\n",
      "\tvalidate_harmeme_intensity_average: 0.6688400506973267\n",
      "\n",
      "\n",
      "Epoch 7: 100%|█| 95/95 [01:02<00:00,  1.53it/s, v_num=10, validate_harmeme_inten\u001b[A\n",
      "[INFO] - Evaluating model - validate...\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]\n",
      "Validation DataLoader 0: 100%|████████████████████| 6/6 [00:01<00:00,  5.50it/s][INFO] - Epoch Results:\n",
      "\tvalidate_harmeme_intensity_accuracy: 0.6440678238868713\n",
      "\tvalidate_harmeme_intensity_auroc: 0.6936122179031372\n",
      "\tvalidate_harmeme_intensity_average: 0.6688400506973267\n",
      "\n",
      "Validation DataLoader 0: 100%|████████████████████| 6/6 [00:01<00:00,  5.47it/s]\n",
      "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
      "┃\u001b[1m \u001b[0m\u001b[1m          Validate metric          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m           DataLoader 0            \u001b[0m\u001b[1m \u001b[0m┃\n",
      "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
      "│\u001b[36m \u001b[0m\u001b[36mvalidate_harmeme_intensity_accuracy\u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m        0.6440678238868713         \u001b[0m\u001b[35m \u001b[0m│\n",
      "│\u001b[36m \u001b[0m\u001b[36m validate_harmeme_intensity_auroc  \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m        0.6936122179031372         \u001b[0m\u001b[35m \u001b[0m│\n",
      "│\u001b[36m \u001b[0m\u001b[36mvalidate_harmeme_intensity_average \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m        0.6688400506973267         \u001b[0m\u001b[35m \u001b[0m│\n",
      "│\u001b[36m \u001b[0m\u001b[36m           validate_loss           \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m         1.066281795501709         \u001b[0m\u001b[35m \u001b[0m│\n",
      "└─────────────────────────────────────┴─────────────────────────────────────┘\n",
      "[INFO] - Evaluating model - test...\n",
      "Dataset preprocessing: 100%|██████████████| 354/354 [00:00<00:00, 635609.42it/s]\n",
      "Loading auxiliary info: 100%|██████████████| 354/354 [00:00<00:00, 52540.11it/s]\n",
      "Input/Output formatting: 100%|███████████| 354/354 [00:00<00:00, 1535453.58it/s]\n",
      "Loading images: 100%|████████████████████████| 354/354 [00:01<00:00, 333.49it/s]\n",
      "Restoring states from the checkpoint path at /mnt/data1/mshee/test-repository/MATK/examples/experiments/baseline/harmeme/flava/epoch=2-step=285.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]\n",
      "Loaded model weights from the checkpoint at /mnt/data1/mshee/test-repository/MATK/examples/experiments/baseline/harmeme/flava/epoch=2-step=285.ckpt\n",
      "Testing DataLoader 0: 100%|█████████████████████| 12/12 [00:02<00:00,  5.16it/s][INFO] - Epoch Results:\n",
      "\ttest_harmeme_intensity_accuracy: 0.8107344508171082\n",
      "\ttest_harmeme_intensity_auroc: 0.8663394451141357\n",
      "\ttest_harmeme_intensity_average: 0.8385369777679443\n",
      "\n",
      "Testing DataLoader 0: 100%|█████████████████████| 12/12 [00:02<00:00,  5.15it/s]\n",
      "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
      "┃\u001b[1m \u001b[0m\u001b[1m          Test metric          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m         DataLoader 0          \u001b[0m\u001b[1m \u001b[0m┃\n",
      "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
      "│\u001b[36m \u001b[0m\u001b[36mtest_harmeme_intensity_accuracy\u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m      0.8107344508171082       \u001b[0m\u001b[35m \u001b[0m│\n",
      "│\u001b[36m \u001b[0m\u001b[36m test_harmeme_intensity_auroc  \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m      0.8663394451141357       \u001b[0m\u001b[35m \u001b[0m│\n",
      "│\u001b[36m \u001b[0m\u001b[36mtest_harmeme_intensity_average \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m      0.8385369777679443       \u001b[0m\u001b[35m \u001b[0m│\n",
      "└─────────────────────────────────┴─────────────────────────────────┘\n"
     ]
    }
   ],
   "source": [
    "!python3 ../main.py \\\n",
    "    +experiment=harm_p/flava.yaml \\\n",
    "    action=fit \\\n",
    "    trainer=single_gpu_trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b15d15",
   "metadata": {},
   "source": [
    "## Step 5: Test the Model model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "96b3df1e-df5f-4bdb-b6d7-bd8b5498675f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] - Setting custom seed: 1111...\n",
      "Seed set to 1111\n",
      "/home/mshee/anaconda3/envs/MATK-test/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at facebook/flava-full were not used when initializing FlavaModel: ['mmm_image_head.transform.dense.bias', 'image_codebook.blocks.group_3.group.block_2.res_path.path.conv_2.weight', 'image_codebook.blocks.group_4.group.block_1.id_path.bias', 'image_codebook.blocks.group_4.group.block_1.res_path.path.conv_4.weight', 'image_codebook.blocks.group_3.group.block_2.res_path.path.conv_2.bias', 'mlm_head.transform.dense.bias', 'mim_head.transform.LayerNorm.weight', 'image_codebook.blocks.group_4.group.block_1.res_path.path.conv_2.weight', 'image_codebook.blocks.group_3.group.block_2.res_path.path.conv_3.weight', 'image_codebook.blocks.group_2.group.block_1.res_path.path.conv_1.bias', 'image_codebook.blocks.group_4.group.block_1.res_path.path.conv_1.bias', 'mmm_image_head.decoder.weight', 'mim_head.transform.dense.weight', 'image_codebook.blocks.input.weight', 'mmm_image_head.transform.dense.weight', 'image_codebook.blocks.group_1.group.block_2.res_path.path.conv_1.bias', 'mmm_image_head.transform.LayerNorm.weight', 'mmm_text_head.transform.dense.weight', 'image_codebook.blocks.group_1.group.block_1.res_path.path.conv_4.weight', 'mim_head.decoder.weight', 'mim_head.bias', 'image_codebook.blocks.group_1.group.block_1.res_path.path.conv_4.bias', 'image_codebook.blocks.group_3.group.block_2.res_path.path.conv_4.weight', 'itm_head.pooler.dense.bias', 'image_codebook.blocks.group_4.group.block_2.res_path.path.conv_1.weight', 'image_codebook.blocks.group_4.group.block_1.res_path.path.conv_1.weight', 'image_codebook.blocks.group_2.group.block_1.res_path.path.conv_1.weight', 'image_codebook.blocks.group_2.group.block_2.res_path.path.conv_2.weight', 'mmm_text_head.transform.LayerNorm.bias', 'image_codebook.blocks.group_3.group.block_1.res_path.path.conv_4.bias', 'image_codebook.blocks.group_3.group.block_1.res_path.path.conv_4.weight', 'itm_head.pooler.dense.weight', 'image_codebook.blocks.group_2.group.block_1.res_path.path.conv_2.weight', 'mlm_head.transform.LayerNorm.bias', 'image_codebook.blocks.group_3.group.block_2.res_path.path.conv_1.bias', 'itm_head.seq_relationship.bias', 'image_codebook.blocks.group_4.group.block_1.res_path.path.conv_2.bias', 'image_codebook.blocks.group_4.group.block_1.id_path.weight', 'image_codebook.blocks.group_3.group.block_1.res_path.path.conv_1.weight', 'image_codebook.blocks.group_1.group.block_1.res_path.path.conv_1.weight', 'image_codebook.blocks.group_3.group.block_1.res_path.path.conv_1.bias', 'image_codebook.blocks.group_4.group.block_2.res_path.path.conv_2.bias', 'image_codebook.blocks.group_3.group.block_1.res_path.path.conv_2.weight', 'image_codebook.blocks.group_2.group.block_1.res_path.path.conv_3.bias', 'mmm_text_head.bias', 'mmm_text_head.transform.dense.bias', 'mlm_head.transform.LayerNorm.weight', 'image_codebook.blocks.group_2.group.block_2.res_path.path.conv_2.bias', 'image_codebook.blocks.group_3.group.block_1.id_path.bias', 'image_codebook.blocks.group_4.group.block_1.res_path.path.conv_3.bias', 'image_codebook.blocks.group_1.group.block_1.res_path.path.conv_2.bias', 'image_codebook.blocks.group_2.group.block_2.res_path.path.conv_4.weight', 'image_codebook.blocks.group_3.group.block_1.res_path.path.conv_3.bias', 'mim_head.transform.LayerNorm.bias', 'image_codebook.blocks.group_2.group.block_1.res_path.path.conv_4.weight', 'image_codebook.blocks.output.conv.weight', 'image_codebook.blocks.group_3.group.block_1.id_path.weight', 'image_codebook.blocks.group_1.group.block_2.res_path.path.conv_3.weight', 'image_codebook.blocks.group_3.group.block_1.res_path.path.conv_3.weight', 'image_codebook.blocks.group_2.group.block_1.res_path.path.conv_4.bias', 'image_codebook.blocks.group_4.group.block_2.res_path.path.conv_2.weight', 'image_codebook.blocks.group_2.group.block_1.res_path.path.conv_2.bias', 'mmm_text_head.transform.LayerNorm.weight', 'mlm_head.decoder.bias', 'mmm_image_head.decoder.bias', 'mmm_image_head.bias', 'image_codebook.blocks.group_4.group.block_2.res_path.path.conv_4.weight', 'image_codebook.blocks.group_4.group.block_2.res_path.path.conv_4.bias', 'mmm_image_head.transform.LayerNorm.bias', 'image_codebook.blocks.group_1.group.block_2.res_path.path.conv_3.bias', 'mim_head.decoder.bias', 'image_codebook.blocks.group_1.group.block_2.res_path.path.conv_2.bias', 'mim_head.transform.dense.bias', 'image_codebook.blocks.group_1.group.block_1.res_path.path.conv_3.bias', 'image_codebook.blocks.input.bias', 'image_codebook.blocks.group_2.group.block_1.res_path.path.conv_3.weight', 'image_codebook.blocks.group_1.group.block_2.res_path.path.conv_4.bias', 'image_codebook.blocks.group_1.group.block_1.res_path.path.conv_2.weight', 'image_codebook.blocks.group_1.group.block_2.res_path.path.conv_1.weight', 'image_codebook.blocks.group_1.group.block_2.res_path.path.conv_4.weight', 'mlm_head.bias', 'image_codebook.blocks.group_2.group.block_2.res_path.path.conv_1.weight', 'mmm_text_head.decoder.bias', 'mlm_head.decoder.weight', 'image_codebook.blocks.group_3.group.block_2.res_path.path.conv_4.bias', 'image_codebook.blocks.group_1.group.block_1.res_path.path.conv_1.bias', 'image_codebook.blocks.group_4.group.block_2.res_path.path.conv_1.bias', 'image_codebook.blocks.group_3.group.block_2.res_path.path.conv_3.bias', 'image_codebook.blocks.group_3.group.block_2.res_path.path.conv_1.weight', 'mlm_head.transform.dense.weight', 'image_codebook.blocks.group_2.group.block_1.id_path.bias', 'image_codebook.blocks.group_1.group.block_2.res_path.path.conv_2.weight', 'image_codebook.blocks.group_2.group.block_2.res_path.path.conv_3.bias', 'image_codebook.blocks.group_2.group.block_2.res_path.path.conv_1.bias', 'image_codebook.blocks.group_4.group.block_2.res_path.path.conv_3.weight', 'itm_head.seq_relationship.weight', 'image_codebook.blocks.output.conv.bias', 'image_codebook.blocks.group_2.group.block_2.res_path.path.conv_4.bias', 'image_codebook.blocks.group_4.group.block_1.res_path.path.conv_3.weight', 'mmm_text_head.decoder.weight', 'image_codebook.blocks.group_4.group.block_1.res_path.path.conv_4.bias', 'image_codebook.blocks.group_2.group.block_1.id_path.weight', 'image_codebook.blocks.group_3.group.block_1.res_path.path.conv_2.bias', 'image_codebook.blocks.group_1.group.block_1.res_path.path.conv_3.weight', 'image_codebook.blocks.group_4.group.block_2.res_path.path.conv_3.bias', 'image_codebook.blocks.group_2.group.block_2.res_path.path.conv_3.weight']\n",
      "- This IS expected if you are initializing FlavaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing FlavaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "[INFO] - Model's Parameters: 241357827\n",
      "[INFO] - Model's Trainable Parameters: 241357827\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "[INFO] - Evaluating model...\n",
      "Some weights of the model checkpoint at facebook/flava-full were not used when initializing FlavaModel: ['mmm_image_head.transform.dense.bias', 'image_codebook.blocks.group_3.group.block_2.res_path.path.conv_2.weight', 'image_codebook.blocks.group_4.group.block_1.id_path.bias', 'image_codebook.blocks.group_4.group.block_1.res_path.path.conv_4.weight', 'image_codebook.blocks.group_3.group.block_2.res_path.path.conv_2.bias', 'mlm_head.transform.dense.bias', 'mim_head.transform.LayerNorm.weight', 'image_codebook.blocks.group_4.group.block_1.res_path.path.conv_2.weight', 'image_codebook.blocks.group_3.group.block_2.res_path.path.conv_3.weight', 'image_codebook.blocks.group_2.group.block_1.res_path.path.conv_1.bias', 'image_codebook.blocks.group_4.group.block_1.res_path.path.conv_1.bias', 'mmm_image_head.decoder.weight', 'mim_head.transform.dense.weight', 'image_codebook.blocks.input.weight', 'mmm_image_head.transform.dense.weight', 'image_codebook.blocks.group_1.group.block_2.res_path.path.conv_1.bias', 'mmm_image_head.transform.LayerNorm.weight', 'mmm_text_head.transform.dense.weight', 'image_codebook.blocks.group_1.group.block_1.res_path.path.conv_4.weight', 'mim_head.decoder.weight', 'mim_head.bias', 'image_codebook.blocks.group_1.group.block_1.res_path.path.conv_4.bias', 'image_codebook.blocks.group_3.group.block_2.res_path.path.conv_4.weight', 'itm_head.pooler.dense.bias', 'image_codebook.blocks.group_4.group.block_2.res_path.path.conv_1.weight', 'image_codebook.blocks.group_4.group.block_1.res_path.path.conv_1.weight', 'image_codebook.blocks.group_2.group.block_1.res_path.path.conv_1.weight', 'image_codebook.blocks.group_2.group.block_2.res_path.path.conv_2.weight', 'mmm_text_head.transform.LayerNorm.bias', 'image_codebook.blocks.group_3.group.block_1.res_path.path.conv_4.bias', 'image_codebook.blocks.group_3.group.block_1.res_path.path.conv_4.weight', 'itm_head.pooler.dense.weight', 'image_codebook.blocks.group_2.group.block_1.res_path.path.conv_2.weight', 'mlm_head.transform.LayerNorm.bias', 'image_codebook.blocks.group_3.group.block_2.res_path.path.conv_1.bias', 'itm_head.seq_relationship.bias', 'image_codebook.blocks.group_4.group.block_1.res_path.path.conv_2.bias', 'image_codebook.blocks.group_4.group.block_1.id_path.weight', 'image_codebook.blocks.group_3.group.block_1.res_path.path.conv_1.weight', 'image_codebook.blocks.group_1.group.block_1.res_path.path.conv_1.weight', 'image_codebook.blocks.group_3.group.block_1.res_path.path.conv_1.bias', 'image_codebook.blocks.group_4.group.block_2.res_path.path.conv_2.bias', 'image_codebook.blocks.group_3.group.block_1.res_path.path.conv_2.weight', 'image_codebook.blocks.group_2.group.block_1.res_path.path.conv_3.bias', 'mmm_text_head.bias', 'mmm_text_head.transform.dense.bias', 'mlm_head.transform.LayerNorm.weight', 'image_codebook.blocks.group_2.group.block_2.res_path.path.conv_2.bias', 'image_codebook.blocks.group_3.group.block_1.id_path.bias', 'image_codebook.blocks.group_4.group.block_1.res_path.path.conv_3.bias', 'image_codebook.blocks.group_1.group.block_1.res_path.path.conv_2.bias', 'image_codebook.blocks.group_2.group.block_2.res_path.path.conv_4.weight', 'image_codebook.blocks.group_3.group.block_1.res_path.path.conv_3.bias', 'mim_head.transform.LayerNorm.bias', 'image_codebook.blocks.group_2.group.block_1.res_path.path.conv_4.weight', 'image_codebook.blocks.output.conv.weight', 'image_codebook.blocks.group_3.group.block_1.id_path.weight', 'image_codebook.blocks.group_1.group.block_2.res_path.path.conv_3.weight', 'image_codebook.blocks.group_3.group.block_1.res_path.path.conv_3.weight', 'image_codebook.blocks.group_2.group.block_1.res_path.path.conv_4.bias', 'image_codebook.blocks.group_4.group.block_2.res_path.path.conv_2.weight', 'image_codebook.blocks.group_2.group.block_1.res_path.path.conv_2.bias', 'mmm_text_head.transform.LayerNorm.weight', 'mlm_head.decoder.bias', 'mmm_image_head.decoder.bias', 'mmm_image_head.bias', 'image_codebook.blocks.group_4.group.block_2.res_path.path.conv_4.weight', 'image_codebook.blocks.group_4.group.block_2.res_path.path.conv_4.bias', 'mmm_image_head.transform.LayerNorm.bias', 'image_codebook.blocks.group_1.group.block_2.res_path.path.conv_3.bias', 'mim_head.decoder.bias', 'image_codebook.blocks.group_1.group.block_2.res_path.path.conv_2.bias', 'mim_head.transform.dense.bias', 'image_codebook.blocks.group_1.group.block_1.res_path.path.conv_3.bias', 'image_codebook.blocks.input.bias', 'image_codebook.blocks.group_2.group.block_1.res_path.path.conv_3.weight', 'image_codebook.blocks.group_1.group.block_2.res_path.path.conv_4.bias', 'image_codebook.blocks.group_1.group.block_1.res_path.path.conv_2.weight', 'image_codebook.blocks.group_1.group.block_2.res_path.path.conv_1.weight', 'image_codebook.blocks.group_1.group.block_2.res_path.path.conv_4.weight', 'mlm_head.bias', 'image_codebook.blocks.group_2.group.block_2.res_path.path.conv_1.weight', 'mmm_text_head.decoder.bias', 'mlm_head.decoder.weight', 'image_codebook.blocks.group_3.group.block_2.res_path.path.conv_4.bias', 'image_codebook.blocks.group_1.group.block_1.res_path.path.conv_1.bias', 'image_codebook.blocks.group_4.group.block_2.res_path.path.conv_1.bias', 'image_codebook.blocks.group_3.group.block_2.res_path.path.conv_3.bias', 'image_codebook.blocks.group_3.group.block_2.res_path.path.conv_1.weight', 'mlm_head.transform.dense.weight', 'image_codebook.blocks.group_2.group.block_1.id_path.bias', 'image_codebook.blocks.group_1.group.block_2.res_path.path.conv_2.weight', 'image_codebook.blocks.group_2.group.block_2.res_path.path.conv_3.bias', 'image_codebook.blocks.group_2.group.block_2.res_path.path.conv_1.bias', 'image_codebook.blocks.group_4.group.block_2.res_path.path.conv_3.weight', 'itm_head.seq_relationship.weight', 'image_codebook.blocks.output.conv.bias', 'image_codebook.blocks.group_2.group.block_2.res_path.path.conv_4.bias', 'image_codebook.blocks.group_4.group.block_1.res_path.path.conv_3.weight', 'mmm_text_head.decoder.weight', 'image_codebook.blocks.group_4.group.block_1.res_path.path.conv_4.bias', 'image_codebook.blocks.group_2.group.block_1.id_path.weight', 'image_codebook.blocks.group_3.group.block_1.res_path.path.conv_2.bias', 'image_codebook.blocks.group_1.group.block_1.res_path.path.conv_3.weight', 'image_codebook.blocks.group_4.group.block_2.res_path.path.conv_3.bias', 'image_codebook.blocks.group_2.group.block_2.res_path.path.conv_3.weight']\n",
      "- This IS expected if you are initializing FlavaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing FlavaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "You are using a CUDA device ('NVIDIA A100 80GB PCIe') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "Missing logger folder: lightning_logs/baseline/harmeme/flava/+model_checkpoint=/mnt/data1/mshee/test-repository/MATK/examples/experiments/baseline/harm_p/flava/epoch\\=2-step\\=285.ckpt,trainer=single_gpu_trainer\n",
      "Dataset preprocessing: 100%|██████████████| 354/354 [00:00<00:00, 643878.41it/s]\n",
      "Loading auxiliary info: 100%|██████████████| 354/354 [00:00<00:00, 52465.85it/s]\n",
      "Input/Output formatting: 100%|███████████| 354/354 [00:00<00:00, 1591407.95it/s]\n",
      "Loading images: 100%|████████████████████████| 354/354 [00:01<00:00, 314.99it/s]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]\n",
      "Testing DataLoader 0:   0%|                              | 0/12 [00:00<?, ?it/s]/home/mshee/anaconda3/envs/MATK-test/lib/python3.8/site-packages/transformers/modeling_utils.py:810: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Testing DataLoader 0:  75%|████████████████▌     | 9/12 [00:02<00:00,  3.82it/s]/home/mshee/anaconda3/envs/MATK-test/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: No negative samples in targets, false positive value should be meaningless. Returning zero tensor in false positive score\n",
      "  warnings.warn(*args, **kwargs)\n",
      "/home/mshee/anaconda3/envs/MATK-test/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Testing DataLoader 0: 100%|█████████████████████| 12/12 [00:02<00:00,  4.35it/s][INFO] - Epoch Results:\n",
      "\ttest_harmeme_intensity_accuracy: 0.8107344508171082\n",
      "\ttest_harmeme_intensity_auroc: 0.8663394451141357\n",
      "\ttest_harmeme_intensity_average: 0.8385369777679443\n",
      "\n",
      "Testing DataLoader 0: 100%|█████████████████████| 12/12 [00:02<00:00,  4.33it/s]\n",
      "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
      "┃\u001b[1m \u001b[0m\u001b[1m          Test metric          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m         DataLoader 0          \u001b[0m\u001b[1m \u001b[0m┃\n",
      "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
      "│\u001b[36m \u001b[0m\u001b[36mtest_harmeme_intensity_accuracy\u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m      0.8107344508171082       \u001b[0m\u001b[35m \u001b[0m│\n",
      "│\u001b[36m \u001b[0m\u001b[36m test_harmeme_intensity_auroc  \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m      0.8663394451141357       \u001b[0m\u001b[35m \u001b[0m│\n",
      "│\u001b[36m \u001b[0m\u001b[36mtest_harmeme_intensity_average \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m      0.8385369777679443       \u001b[0m\u001b[35m \u001b[0m│\n",
      "└─────────────────────────────────┴─────────────────────────────────┘\n"
     ]
    }
   ],
   "source": [
    "!python3 ../main.py \\\n",
    "    +experiment=\"harm_p/flava.yaml\" \\\n",
    "    +model_checkpoint=\"/mnt/data1/mshee/test-repository/MATK/examples/experiments/baseline/harm_p/flava/epoch\\=2-step\\=285.ckpt\" \\\n",
    "    trainer=single_gpu_trainer \\\n",
    "    action=test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
