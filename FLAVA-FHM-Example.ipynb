{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7586a2f3",
   "metadata": {},
   "source": [
    "# Example: FLAVA training and inference on FHM dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "45f5ca34",
   "metadata": {},
   "source": [
    "This notebook provides a comprehensive guide on using the MATK (Multimodal AI Toolkit) library to evaluate the performance of the FLAVA model on the Facebook Hateful Memes dataset.\n",
    "\n",
    "We kindly request that interested researchers duly acknowledge and adhere to Facebook AI's Hateful Memes dataset licence agreements. This entails the requisite download of the original dataset provided by Facebook AI."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d38242a1",
   "metadata": {},
   "source": [
    "## Step 1. Review and Accept Facebook AI's Dataset Licence Agreement\n",
    "Researchers may access the Hateful Memes dataset license agreements by visiting the official website at https://hatefulmemeschallenge.com/. Once researchers have carefully reviewed and duly accepted the terms outlined in the license agreements, they are eligible to proceed with the download of the Hateful Memes datasets. This includes\n",
    "\n",
    "* train, dev, dev_seen and test annotations\n",
    "* images (critical for vision-language multimodal models)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ac430990",
   "metadata": {},
   "source": [
    "## Step 2. Configuring the dataset\n",
    "\n",
    "1. Locate the **configs/datasets.yaml** file. Enter the paths for 'annotation_filepaths', 'image_dirs', 'feats_dirs' depending on your requirement. For FLAVA, we need 'annotation_filepaths' and 'image_dirs' since our ImagesDataModule needs these arguments\n",
    "\n",
    "2. Next locate the **configs/data** folder. We will use the fhm_data.yaml file because we are using the FHM dataset.\n",
    "\n",
    "3. Inside the fhm_data.yaml you should see various 'datamodules' that are also listed in the table below. \n",
    "\n",
    "| Dataset              | DataModule        | Usage                      |\n",
    "|----------------------|-------------------|----------------------------|\n",
    "| FasterRCNNDataModule | FasterRCNNDataset | For vision-language models |\n",
    "| ImagesDataModule     | ImagesDataset     | For vision-language models |\n",
    "| TextDataModule       | TextDataset       | For language models        |\n",
    "\n",
    "\n",
    "2. **tokenizer_class_or_path**: specifies tokenizer or processor class/path for model\n",
    "3. **frcnn_class_or_path**: specifies class/path Faster R-CNN feature extraction\n",
    "4. **dataset_class**: specifies the dataset class to use for the current datamodule\n",
    "5. **dataset_handler**: specifies the file to be passed to the dataset_handler so that the dataset class knows where to get its data from\n",
    "6. **auxiliary_dicts**: path to .pkl containing auxiliary information like captions for images\n",
    "8. **num_workers**: perform multi-process data loading by simply setting the argument num_workers to a positive integer\n",
    "\n",
    "\n",
    "### Modification\n",
    "\n",
    "1. You can suitably modify the **batch_size**, **num_workers**  and **shuffle_train** arguments based on your need."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3703ec20",
   "metadata": {},
   "source": [
    "## Step 3. Configuring the model-to-dataset mapping\n",
    "\n",
    "Our model needs to know some information about the data it is going to handle so that it can appropriately initiate metrics calculation and logging.\n",
    "\n",
    "1. Locate the **fhm** key\n",
    "2. Locate the **flava** key within the above key\n",
    "3. Inside cls_dict we specify 'label: 2' because our dataset has exactly 1 label called 'label' and it can take 2 values - 0 or 1. Another example is that the FHM finegrained dataset has exactly 1 label called 'hate' and it can take 2 values - 0 or 1."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2d3b9927",
   "metadata": {},
   "source": [
    "## Step 4: Configuring the Model\n",
    "\n",
    "Everything related to the model configuration is stored inside the **configs/models.yaml** file.\n",
    "\n",
    "For the **flava** key we specify the following arguments:\n",
    "1. **class_path**: specifies path to file under **[models/](https://github.com/Social-AI-Studio/MATK/tree/main/models)**\n",
    "2. **model_class_or_path**: specifies the pretrained model to be used\n",
    "3. **metrics** (only for VL Models) - List of metrics from torchmetrics, each element specifies the torchmetrics metric name, task and num_classes. \n",
    "\n",
    "for further metric configuration, you can look at the **[link](https://torchmetrics.readthedocs.io/en/stable/all-metrics.html)** to add more arguments inside the **args** of list of each metric in metrics\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "289e0486",
   "metadata": {},
   "source": [
    "## Step 5: Configuring the Trainer\n",
    "\n",
    "The Trainer helps automate several aspects of training. As the documentation says,  it handles all loop details for you, some examples include:\n",
    "* Automatically enabling/disabling grads\n",
    "* Running the training, validation and test dataloaders\n",
    "* Calling the Callbacks at the appropriate times\n",
    "* Putting batches and computations on the correct devices\n",
    "\n",
    "Everything related to the trainer is specified under the **{dataset}_{task}_trainer.yaml** file inside the **configs/trainers** folder.\n",
    "\n",
    "### Modification\n",
    "1. Suitably modify **dirpath** and **name** arguments under callbacks to choose where your checkpoints will be stored and what name it will be given respectively. \n",
    "2. Suitably modify **save_dir** and **name** arguments under logger to choose where your lightining logs will be stored and what name it will be given respectively.\n",
    "3. To add arguments like 'seed_everything' or 'ckpt_path', add them at the same level as the **trainer** key.\n",
    "3. You can also modify other hyperparameters such as **max_epochs** or even find new ways to tweak the trainer (within the **trainer** key) by adding keys mentioned here: https://lightning.ai/docs/pytorch/stable/common/trainer.html#\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c6e954e8",
   "metadata": {},
   "source": [
    "## Step 6: Train the Model model\n",
    "\n",
    "Make sure you have already run :\n",
    "```\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "Then run:\n",
    "```\n",
    "python main.py --model flava --dataset fhm --datamodule ImagesDataModule --action fit\n",
    "```\n",
    "\n",
    "--model => from the keys under **models** key in models.yaml \\\n",
    "--fhm => from the keys under **datasets** key in datasets.yaml \\\n",
    "--datamodule => from the keys under the {dataset}_{task}_data.yaml file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b15d15",
   "metadata": {},
   "source": [
    "## Step 7: Test the Model model\n",
    "\n",
    "Remember to create a key called **ckpt_path** at the same level as the **trainer** key under **flava** in the configs/fhm_trainer.yaml file. Look under the **filename** key of your ModelCheckpoint callback for this name.\n",
    "\n",
    "Then run:\n",
    "```\n",
    "python main.py --model flava --dataset fhm --datamodule ImagesDataModule --action test\n",
    "```\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
